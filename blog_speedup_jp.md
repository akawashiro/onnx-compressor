## MNISTを可能な限り高速に分類する

概要
- MNISTの分類をする学習済みモデルを軽量化し、
- 更にSIMD命令を使った高速化を行うことで、
- シングルスレッドに限れば`onnxruntime`より高速なMNISTの分類が可能になった

この記事は[前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)の続きです。
前回ではMNISTを分類する学習済みニューラルネットワークから不要な枝を削除し、軽量化した学習済みモデルを走らせる専用のランタイムを作ってMNIST分類の高速化を試みました。
今回はSIMD命令とマルチスレッド化でMNIST分類速度の限界に挑みます。

今回の目標タイムは既存のONNXランタイム[onnxruntime](https://github.com/microsoft/onnxruntime)です。
この記事の各実行時間の計測は10回行い、その平均と分散を求めました。
onnxruntimeの実行時間はシングルスレッドで1.259秒、マルチスレッドで0.505秒です。[^1]
実行環境はOS: Ubuntu19.04, CPU: Core i7 8th Gen, メモリ: 16GiBです。
|------------------|----------------------------|
| シングルスレッド | 1.259秒(標準偏差0.1148秒)  |
| マルチスレッド   | 0.505秒(標準偏差0.04249秒) |
|------------------|----------------------------|
[^1]: [前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)からOSを入れ替えたので数値が違います。


機械学習の推論過程、特にMNISTを分類する場合、においてボトルネックになるのは重み行列を入力ベクトルに乗算する処理です。
[前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)は重み行列の中で絶対値の小さい要素を無視することで計算の効率化を図り、行列の要素の80%を無視して約5倍の高速化に成功しました。
しかし、80%を削減してもなお乗算はボトルネックでした。

具体的には[sonnx.cpp](https://github.com/akawashiro/sonnx/blob/master/sonnx.cpp)のこの部分がボトルネックになります。
```C++
for(int i=0;i<n;i++){
        ret[B_row[i]] += B_scale[i] * x[B_column[i]];
}
```

まず、SIMDで高速化してみます。
Core i7 8th GenではAVX2命令セットが使えるので256bitの演算を一度に行うことができ、
今回は32bit浮動小数点数で計算しているので最大8倍の高速化が見込めます。

しかし、元のコードはメモリへの間接参照を２つ(`ret[B_row[i]]`と`x[B_column[i]]`)含んでおりそのままではSIMD化するのが困難です。
まず`ret[B_row[i]]`への書き込みは同じ`B_row[i]`の値を持つものをまとめて計算し、メモリへの書き込みを一度に行います。
`x[B_column[i]]`からの読み出しはAVX2のgather命令を使ってSIMD化します。
完成したコードがこれです。

```C++
float r = 0;
__m256 vr = _mm256_setzero_ps();
for(cur;cur<n1;cur+=8){
    __m256i vc = _mm256_loadu_si256((__m256i*)(&B_column_p[cur]));
    __m256 vx = _mm256_i32gather_ps(x_p, vc, 4);
    __m256 vs = _mm256_load_ps(&B_scale_p[cur]);
    vr = _mm256_fmadd_ps(vs, vx, vr);
}
for(cur;cur<n;cur++){
    r += B_scale_p[cur] * x_p[B_column_p[cur]];
}
__attribute__((aligned(32))) float t[8] = {0};
_mm256_store_ps(t, vr);
ret[B_row_p[cur-1]] += r + t[0] + t[1] + t[2] + t[3] + t[4] + t[5] + t[6] + t[7];
```

実行時間はこんな感じになりました。

|-------------------------------|----------------------------|
| SIMD                          | 1.121秒(標準偏差0.02271秒) |
| onnxruntime(シングルスレッド) | 1.259秒(標準偏差0.1148秒)  |
|-------------------------------|----------------------------|

シングルスレッドでの性能においては[onnxruntime](https://github.com/microsoft/onnxruntime)に勝ったと言えます。

(SIMDなしの)マルチスレッド化はどうでしょうか。ボトルネックになっているコードはメモリへの書き込みが一箇所(`ret[B_row[i]]`への書き込み)しかないので、この部分だけ複数スレッドで同時に書き込まないようにすればデータレースは起こりません。結果は図1のようになりました。

スレッド数を増やせばいいってもんではないみたいですね。コンテキストスイッチの処理時間とかのせいでしょうか。

では最後にSIMDとマルチスレッディングを併用してみます。
