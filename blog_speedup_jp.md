## MNISTを可能な限り高速に分類する

概要
- MNISTの分類をする学習済みモデルを軽量化し、
- 更にSIMD命令を使った高速化を行うことで、
- シングルスレッドに限れば`onnxruntime`より高速なMNISTの分類が可能になった

この記事は[前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)の続きです。
前回ではMNISTを分類する学習済みニューラルネットワークから不要な枝を削除し、軽量化した学習済みモデルを走らせる専用のランタイムを作ってMNIST分類の高速化を試みました。
今回はSIMD命令とマルチスレッド化でMNIST分類速度の限界に挑みます。

今回の目標タイムは既存のONNXランタイム[onnxruntime](https://github.com/microsoft/onnxruntime)です。
onnxruntimeの実行時間はシングルスレッドで1.259秒、マルチスレッドで0.505秒です。[^1]
|------------------|-------------------|
| シングルスレッド | 1.259秒(標準偏差) |
| マルチスレッド   | 0.505秒(標準偏差) |
|------------------|-------------------|
[^1]: [前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)からOSを入れ替えたので数値が違います。


機械学習の推論過程、特にMNISTを分類する場合、においてボトルネックになるのは重み行列を入力ベクトルに乗算する処理です。
[前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)は重み行列の中で絶対値の小さい要素を無視することで計算の効率化を図り、行列の要素の80%を無視して約5倍の高速化に成功しました。
しかし、80%を削減してもなお乗算はボトルネックでした。

具体的には[sonnx.cpp](https://github.com/akawashiro/sonnx/blob/master/sonnx.cpp)のこの部分がボトルネックになります。
```C++
for(int i=0;i<n;i++){
        ret[B_row[i]] += B_scale[i] * x[B_column[i]];
}
```

まず、SIMDで高速化してみます。僕のPCはAVX2命令セットが使えるので、256bitのベクトル化が可能です。
今回、各要素は32bit浮動小数点数なのでSIMD化によって最大8倍の高速化が見込めます。
では結果です。
実行時間の計測は10回行い、その平均と分散を求めました。

かなり微妙ですがシングルスレッドでの性能においては[onnxruntime](https://github.com/microsoft/onnxruntime)に勝ったと言えると思います。

(SIMDなしの)マルチスレッド化はどうでしょうか。ボトルネックになっているコードはメモリへの書き込みが一箇所(`ret[B_row[i]]`への書き込み)しかないので、この部分だけ複数スレッドで同時に書き込まないようにすればデータレースは起こりません。結果は図1のようになりました。

スレッド数を増やせばいいってもんではないみたいですね。コンテキストスイッチの処理時間とかのせいでしょうか。

では最後にSIMDとマルチスレッディングを併用してみます。
