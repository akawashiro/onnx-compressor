## MNISTを可能な限り高速に分類する

この記事は[前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)の続きです。
前回ではMNISTを分類する学習済みニューラルネットワークから不要な枝を削除し、軽量化した学習済みモデルを走らせる専用のランタイムを作ってMNIST分類の高速化を試みました。
今回はSIMD命令とマルチスレッド化でMNIST分類速度の限界に挑みます。

今回の目標タイムは既存のONNXランタイム[onnxruntime](https://github.com/microsoft/onnxruntime)です。
onnxruntimeの実行時間はシングルスレッドで1.259秒、マルチスレッドで0.505秒です。[^1]
|------------------|---------|
| シングルスレッド | 1.259秒 |
| マルチスレッド   | 0.505秒 |
|------------------|---------|
[^1]: [前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)からOSを入れ替えたので数値が違います。


機械学習の推論過程、特にMNISTを分類する場合、においてボトルネックになるのは重み行列を入力ベクトルに乗算する処理です。
[前回](http://a-kawashiro.hatenablog.com/entry/2019/03/07/201304)は重み行列の中で絶対値の小さい要素を無視することで計算の効率化を図り、行列の要素の80%を無視して約5倍の高速化に成功しました。
しかし、80%を削減してもなお乗算はボトルネックでした。

具体的には[sonnx.cpp](https://github.com/akawashiro/sonnx/blob/master/sonnx.cpp)のこの部分がボトルネックになります。
```C++
for(int i=0;i<n;i++){
        ret[B_row[i]] += B_scale[i] * x[B_column[i]];
}
```

まずはマルチスレッド化です。ボトルネックになっているコードはメモリへの書き込みが一箇所(`ret[B_row[i]]`への書き込み)しかないので、この部分だけ複数スレッドで同時に書き込まないようにすれば大丈夫です。結果は図1のようになりました。

スレッド数を増やせばいいってもんではないみたいですね。コンテキストスイッチの処理時間とかのせいかな。
